{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "path = '/home/ge65cuw/thesis/saved/final_results/results_df.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(path, index_col=0)\n",
    "results_df = results_df.dropna()\n",
    "trials_list = results_df.trial_id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found\n",
      "Not found\n",
      "Not found\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "trials_df = pd.DataFrame()\n",
    "for trial in trials_list:\n",
    "    file_name = str(trial) + '_trial_full.json'\n",
    "    file_path = '/home/ge65cuw/thesis/saved/final_results/trial_info/'\n",
    "    try:\n",
    "        with open(file_path+file_name) as f:\n",
    "            data = json.load(f)\n",
    "            params = data['params']\n",
    "        trial_df = pd.DataFrame(params, index=[0])\n",
    "        trial_df['trial_id'] = trial\n",
    "        trials_df = pd.concat([trials_df, trial_df], ignore_index=True) \n",
    "    except:\n",
    "        print('Not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials_df.loc[(trials_df.act_func != 'Tanh') & (trials_df.act_func != 'Sigmoid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_df = trials_df.drop(['epochs', 'batch_size', 'patience','huber_delta','log_returns', 'hidden_layer6', 'hidden_layer7', 'hidden_layer8', 'hidden_layer9', 'hidden_layer10', 'loss_fn'], axis=1)\n",
    "trials_df['batch_norm'] = trials_df['batch_norm'].fillna(0)\n",
    "trials_df['l1_lambda1'] = trials_df['l1_lambda1'].fillna(0)\n",
    "trials_df['l2_lambda'] = trials_df['l2_lambda'].fillna(0)\n",
    "trials_df['dropout_prob'] = trials_df['dropout_prob'].fillna(0)\n",
    "trials_df['n_layers'] = trials_df.iloc[:,:4].astype(bool).sum(axis=1)\n",
    "trials_df['n_neurons'] = trials_df.iloc[:,:4].sum(axis=1)\n",
    "trials_df = trials_df.loc[(trials_df.act_func != 'Tanh') & (trials_df.act_func != 'Sigmoid')]\n",
    "trials_df.iloc[:,:5] = trials_df.iloc[:,:5].astype(bool).astype(int)\n",
    "\n",
    "# Remove layers info\n",
    "trials_df = trials_df.iloc[:,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_df = trials_df.merge(results_df[['trial_id', 'oosSpearman']], on='trial_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = trials_df.columns.to_list()#.remove(['trial_id', 'oosSpearman'])\n",
    "columns.remove('trial_id')\n",
    "columns.remove('oosSpearman')\n",
    "columns_new = ['oosSpearman']\n",
    "columns_new.extend(columns)\n",
    "trials_df = trials_df[columns_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_df = pd.get_dummies(trials_df, columns=['act_func','optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oosSpearman</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>l1_lambda1</th>\n",
       "      <th>l2_lambda</th>\n",
       "      <th>dropout_prob</th>\n",
       "      <th>batch_norm</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>n_neurons</th>\n",
       "      <th>act_func_LeakyReLU</th>\n",
       "      <th>act_func_ReLU</th>\n",
       "      <th>optimizer_Adadelta</th>\n",
       "      <th>optimizer_Adagrad</th>\n",
       "      <th>optimizer_Adam</th>\n",
       "      <th>optimizer_Nadam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>oosSpearman</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109052</td>\n",
       "      <td>-0.077405</td>\n",
       "      <td>-0.153250</td>\n",
       "      <td>-0.085600</td>\n",
       "      <td>-5.269540e-02</td>\n",
       "      <td>0.052605</td>\n",
       "      <td>0.280161</td>\n",
       "      <td>0.054105</td>\n",
       "      <td>-0.054105</td>\n",
       "      <td>-0.559661</td>\n",
       "      <td>-0.140459</td>\n",
       "      <td>3.095316e-01</td>\n",
       "      <td>0.312694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.109052</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076175</td>\n",
       "      <td>0.081236</td>\n",
       "      <td>-0.042242</td>\n",
       "      <td>1.747999e-01</td>\n",
       "      <td>0.023324</td>\n",
       "      <td>0.253376</td>\n",
       "      <td>0.050904</td>\n",
       "      <td>-0.050904</td>\n",
       "      <td>-0.035884</td>\n",
       "      <td>0.101788</td>\n",
       "      <td>-7.990184e-03</td>\n",
       "      <td>-0.037829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l1_lambda1</th>\n",
       "      <td>-0.077405</td>\n",
       "      <td>0.076175</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.865712</td>\n",
       "      <td>0.367437</td>\n",
       "      <td>9.369168e-02</td>\n",
       "      <td>-0.063499</td>\n",
       "      <td>0.200159</td>\n",
       "      <td>0.010108</td>\n",
       "      <td>-0.010108</td>\n",
       "      <td>-0.001938</td>\n",
       "      <td>0.083866</td>\n",
       "      <td>-4.565723e-02</td>\n",
       "      <td>-0.014968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l2_lambda</th>\n",
       "      <td>-0.153250</td>\n",
       "      <td>0.081236</td>\n",
       "      <td>0.865712</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373618</td>\n",
       "      <td>1.998350e-01</td>\n",
       "      <td>-0.255741</td>\n",
       "      <td>0.148189</td>\n",
       "      <td>-0.099306</td>\n",
       "      <td>0.099306</td>\n",
       "      <td>0.161268</td>\n",
       "      <td>0.056756</td>\n",
       "      <td>-1.023232e-01</td>\n",
       "      <td>-0.088693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout_prob</th>\n",
       "      <td>-0.085600</td>\n",
       "      <td>-0.042242</td>\n",
       "      <td>0.367437</td>\n",
       "      <td>0.373618</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.154731e-02</td>\n",
       "      <td>-0.176281</td>\n",
       "      <td>0.168514</td>\n",
       "      <td>-0.277420</td>\n",
       "      <td>0.277420</td>\n",
       "      <td>0.032440</td>\n",
       "      <td>-0.101052</td>\n",
       "      <td>-9.642622e-02</td>\n",
       "      <td>0.155115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_norm</th>\n",
       "      <td>-0.052695</td>\n",
       "      <td>0.174800</td>\n",
       "      <td>0.093692</td>\n",
       "      <td>0.199835</td>\n",
       "      <td>0.081547</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-0.350647</td>\n",
       "      <td>-0.176853</td>\n",
       "      <td>0.056056</td>\n",
       "      <td>-0.056056</td>\n",
       "      <td>0.037582</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>7.494862e-18</td>\n",
       "      <td>-0.072548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_layers</th>\n",
       "      <td>0.052605</td>\n",
       "      <td>0.023324</td>\n",
       "      <td>-0.063499</td>\n",
       "      <td>-0.255741</td>\n",
       "      <td>-0.176281</td>\n",
       "      <td>-3.506469e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300005</td>\n",
       "      <td>0.100855</td>\n",
       "      <td>-0.100855</td>\n",
       "      <td>0.072707</td>\n",
       "      <td>-0.096730</td>\n",
       "      <td>-7.836039e-02</td>\n",
       "      <td>0.092982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_neurons</th>\n",
       "      <td>0.280161</td>\n",
       "      <td>0.253376</td>\n",
       "      <td>0.200159</td>\n",
       "      <td>0.148189</td>\n",
       "      <td>0.168514</td>\n",
       "      <td>-1.768528e-01</td>\n",
       "      <td>0.300005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.264383</td>\n",
       "      <td>0.264383</td>\n",
       "      <td>-0.136536</td>\n",
       "      <td>0.094999</td>\n",
       "      <td>1.857905e-02</td>\n",
       "      <td>0.035589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>act_func_LeakyReLU</th>\n",
       "      <td>0.054105</td>\n",
       "      <td>0.050904</td>\n",
       "      <td>0.010108</td>\n",
       "      <td>-0.099306</td>\n",
       "      <td>-0.277420</td>\n",
       "      <td>5.605607e-02</td>\n",
       "      <td>0.100855</td>\n",
       "      <td>-0.264383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.059831</td>\n",
       "      <td>-0.022422</td>\n",
       "      <td>3.113870e-01</td>\n",
       "      <td>-0.265964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>act_func_ReLU</th>\n",
       "      <td>-0.054105</td>\n",
       "      <td>-0.050904</td>\n",
       "      <td>-0.010108</td>\n",
       "      <td>0.099306</td>\n",
       "      <td>0.277420</td>\n",
       "      <td>-5.605607e-02</td>\n",
       "      <td>-0.100855</td>\n",
       "      <td>0.264383</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059831</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>-3.113870e-01</td>\n",
       "      <td>0.265964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimizer_Adadelta</th>\n",
       "      <td>-0.559661</td>\n",
       "      <td>-0.035884</td>\n",
       "      <td>-0.001938</td>\n",
       "      <td>0.161268</td>\n",
       "      <td>0.032440</td>\n",
       "      <td>3.758230e-02</td>\n",
       "      <td>0.072707</td>\n",
       "      <td>-0.136536</td>\n",
       "      <td>-0.059831</td>\n",
       "      <td>0.059831</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.225494</td>\n",
       "      <td>-4.175334e-01</td>\n",
       "      <td>-0.327181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimizer_Adagrad</th>\n",
       "      <td>-0.140459</td>\n",
       "      <td>0.101788</td>\n",
       "      <td>0.083866</td>\n",
       "      <td>0.056756</td>\n",
       "      <td>-0.101052</td>\n",
       "      <td>4.545455e-02</td>\n",
       "      <td>-0.096730</td>\n",
       "      <td>0.094999</td>\n",
       "      <td>-0.022422</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>-0.225494</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-3.086067e-01</td>\n",
       "      <td>-0.241825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimizer_Adam</th>\n",
       "      <td>0.309532</td>\n",
       "      <td>-0.007990</td>\n",
       "      <td>-0.045657</td>\n",
       "      <td>-0.102323</td>\n",
       "      <td>-0.096426</td>\n",
       "      <td>7.494862e-18</td>\n",
       "      <td>-0.078360</td>\n",
       "      <td>0.018579</td>\n",
       "      <td>0.311387</td>\n",
       "      <td>-0.311387</td>\n",
       "      <td>-0.417533</td>\n",
       "      <td>-0.308607</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-0.447774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimizer_Nadam</th>\n",
       "      <td>0.312694</td>\n",
       "      <td>-0.037829</td>\n",
       "      <td>-0.014968</td>\n",
       "      <td>-0.088693</td>\n",
       "      <td>0.155115</td>\n",
       "      <td>-7.254763e-02</td>\n",
       "      <td>0.092982</td>\n",
       "      <td>0.035589</td>\n",
       "      <td>-0.265964</td>\n",
       "      <td>0.265964</td>\n",
       "      <td>-0.327181</td>\n",
       "      <td>-0.241825</td>\n",
       "      <td>-4.477737e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    oosSpearman  learning_rate  l1_lambda1  l2_lambda  \\\n",
       "oosSpearman            1.000000       0.109052   -0.077405  -0.153250   \n",
       "learning_rate          0.109052       1.000000    0.076175   0.081236   \n",
       "l1_lambda1            -0.077405       0.076175    1.000000   0.865712   \n",
       "l2_lambda             -0.153250       0.081236    0.865712   1.000000   \n",
       "dropout_prob          -0.085600      -0.042242    0.367437   0.373618   \n",
       "batch_norm            -0.052695       0.174800    0.093692   0.199835   \n",
       "n_layers               0.052605       0.023324   -0.063499  -0.255741   \n",
       "n_neurons              0.280161       0.253376    0.200159   0.148189   \n",
       "act_func_LeakyReLU     0.054105       0.050904    0.010108  -0.099306   \n",
       "act_func_ReLU         -0.054105      -0.050904   -0.010108   0.099306   \n",
       "optimizer_Adadelta    -0.559661      -0.035884   -0.001938   0.161268   \n",
       "optimizer_Adagrad     -0.140459       0.101788    0.083866   0.056756   \n",
       "optimizer_Adam         0.309532      -0.007990   -0.045657  -0.102323   \n",
       "optimizer_Nadam        0.312694      -0.037829   -0.014968  -0.088693   \n",
       "\n",
       "                    dropout_prob    batch_norm  n_layers  n_neurons  \\\n",
       "oosSpearman            -0.085600 -5.269540e-02  0.052605   0.280161   \n",
       "learning_rate          -0.042242  1.747999e-01  0.023324   0.253376   \n",
       "l1_lambda1              0.367437  9.369168e-02 -0.063499   0.200159   \n",
       "l2_lambda               0.373618  1.998350e-01 -0.255741   0.148189   \n",
       "dropout_prob            1.000000  8.154731e-02 -0.176281   0.168514   \n",
       "batch_norm              0.081547  1.000000e+00 -0.350647  -0.176853   \n",
       "n_layers               -0.176281 -3.506469e-01  1.000000   0.300005   \n",
       "n_neurons               0.168514 -1.768528e-01  0.300005   1.000000   \n",
       "act_func_LeakyReLU     -0.277420  5.605607e-02  0.100855  -0.264383   \n",
       "act_func_ReLU           0.277420 -5.605607e-02 -0.100855   0.264383   \n",
       "optimizer_Adadelta      0.032440  3.758230e-02  0.072707  -0.136536   \n",
       "optimizer_Adagrad      -0.101052  4.545455e-02 -0.096730   0.094999   \n",
       "optimizer_Adam         -0.096426  7.494862e-18 -0.078360   0.018579   \n",
       "optimizer_Nadam         0.155115 -7.254763e-02  0.092982   0.035589   \n",
       "\n",
       "                    act_func_LeakyReLU  act_func_ReLU  optimizer_Adadelta  \\\n",
       "oosSpearman                   0.054105      -0.054105           -0.559661   \n",
       "learning_rate                 0.050904      -0.050904           -0.035884   \n",
       "l1_lambda1                    0.010108      -0.010108           -0.001938   \n",
       "l2_lambda                    -0.099306       0.099306            0.161268   \n",
       "dropout_prob                 -0.277420       0.277420            0.032440   \n",
       "batch_norm                    0.056056      -0.056056            0.037582   \n",
       "n_layers                      0.100855      -0.100855            0.072707   \n",
       "n_neurons                    -0.264383       0.264383           -0.136536   \n",
       "act_func_LeakyReLU            1.000000      -1.000000           -0.059831   \n",
       "act_func_ReLU                -1.000000       1.000000            0.059831   \n",
       "optimizer_Adadelta           -0.059831       0.059831            1.000000   \n",
       "optimizer_Adagrad            -0.022422       0.022422           -0.225494   \n",
       "optimizer_Adam                0.311387      -0.311387           -0.417533   \n",
       "optimizer_Nadam              -0.265964       0.265964           -0.327181   \n",
       "\n",
       "                    optimizer_Adagrad  optimizer_Adam  optimizer_Nadam  \n",
       "oosSpearman                 -0.140459    3.095316e-01         0.312694  \n",
       "learning_rate                0.101788   -7.990184e-03        -0.037829  \n",
       "l1_lambda1                   0.083866   -4.565723e-02        -0.014968  \n",
       "l2_lambda                    0.056756   -1.023232e-01        -0.088693  \n",
       "dropout_prob                -0.101052   -9.642622e-02         0.155115  \n",
       "batch_norm                   0.045455    7.494862e-18        -0.072548  \n",
       "n_layers                    -0.096730   -7.836039e-02         0.092982  \n",
       "n_neurons                    0.094999    1.857905e-02         0.035589  \n",
       "act_func_LeakyReLU          -0.022422    3.113870e-01        -0.265964  \n",
       "act_func_ReLU                0.022422   -3.113870e-01         0.265964  \n",
       "optimizer_Adadelta          -0.225494   -4.175334e-01        -0.327181  \n",
       "optimizer_Adagrad            1.000000   -3.086067e-01        -0.241825  \n",
       "optimizer_Adam              -0.308607    1.000000e+00        -0.447774  \n",
       "optimizer_Nadam             -0.241825   -4.477737e-01         1.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On long-short returns        \n",
    "X = trials_df.iloc[:, 1:]\n",
    "\n",
    "# Column 1 is long returns on max quantile, \n",
    "# Column 2 is long-short returns\n",
    "y = trials_df.iloc[:,0]\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "lm = sm.OLS(y, X).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>oosSpearman</td>   <th>  R-squared:         </th> <td>   0.478</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 28 Nov 2022</td> <th>  Prob (F-statistic):</th> <td>4.83e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:18:24</td>     <th>  Log-Likelihood:    </th> <td>  193.99</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    77</td>      <th>  AIC:               </th> <td>  -364.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    65</td>      <th>  BIC:               </th> <td>  -335.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "           <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>              <td>    0.0249</td> <td>    0.016</td> <td>    1.566</td> <td> 0.122</td> <td>   -0.007</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>learning_rate</th>      <td>    3.6644</td> <td>    9.522</td> <td>    0.385</td> <td> 0.702</td> <td>  -15.352</td> <td>   22.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>l1_lambda1</th>         <td>   -0.0236</td> <td>    0.035</td> <td>   -0.679</td> <td> 0.500</td> <td>   -0.093</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>l2_lambda</th>          <td>    0.0108</td> <td>    0.033</td> <td>    0.331</td> <td> 0.742</td> <td>   -0.054</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dropout_prob</th>       <td>   -0.0142</td> <td>    0.012</td> <td>   -1.145</td> <td> 0.256</td> <td>   -0.039</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>batch_norm</th>         <td>    0.0015</td> <td>    0.008</td> <td>    0.200</td> <td> 0.842</td> <td>   -0.014</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_layers</th>           <td>   -0.0033</td> <td>    0.007</td> <td>   -0.457</td> <td> 0.650</td> <td>   -0.018</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_neurons</th>          <td> 4.884e-06</td> <td> 1.81e-06</td> <td>    2.700</td> <td> 0.009</td> <td> 1.27e-06</td> <td>  8.5e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>act_func_LeakyReLU</th> <td>    0.0154</td> <td>    0.009</td> <td>    1.766</td> <td> 0.082</td> <td>   -0.002</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>act_func_ReLU</th>      <td>    0.0094</td> <td>    0.008</td> <td>    1.138</td> <td> 0.259</td> <td>   -0.007</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>optimizer_Adadelta</th> <td>   -0.0161</td> <td>    0.007</td> <td>   -2.249</td> <td> 0.028</td> <td>   -0.030</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>optimizer_Adagrad</th>  <td>   -0.0029</td> <td>    0.006</td> <td>   -0.466</td> <td> 0.643</td> <td>   -0.015</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>optimizer_Adam</th>     <td>    0.0183</td> <td>    0.005</td> <td>    3.561</td> <td> 0.001</td> <td>    0.008</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>optimizer_Nadam</th>    <td>    0.0255</td> <td>    0.006</td> <td>    4.090</td> <td> 0.000</td> <td>    0.013</td> <td>    0.038</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.799</td> <th>  Durbin-Watson:     </th> <td>   1.624</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.407</td> <th>  Jarque-Bera (JB):  </th> <td>   1.359</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.321</td> <th>  Prob(JB):          </th> <td>   0.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.107</td> <th>  Cond. No.          </th> <td>2.52e+19</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 7.32e-31. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:            oosSpearman   R-squared:                       0.478\n",
       "Model:                            OLS   Adj. R-squared:                  0.390\n",
       "Method:                 Least Squares   F-statistic:                     5.416\n",
       "Date:                Mon, 28 Nov 2022   Prob (F-statistic):           4.83e-06\n",
       "Time:                        10:18:24   Log-Likelihood:                 193.99\n",
       "No. Observations:                  77   AIC:                            -364.0\n",
       "Df Residuals:                      65   BIC:                            -335.9\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "======================================================================================\n",
       "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------\n",
       "const                  0.0249      0.016      1.566      0.122      -0.007       0.057\n",
       "learning_rate          3.6644      9.522      0.385      0.702     -15.352      22.681\n",
       "l1_lambda1            -0.0236      0.035     -0.679      0.500      -0.093       0.046\n",
       "l2_lambda              0.0108      0.033      0.331      0.742      -0.054       0.076\n",
       "dropout_prob          -0.0142      0.012     -1.145      0.256      -0.039       0.011\n",
       "batch_norm             0.0015      0.008      0.200      0.842      -0.014       0.017\n",
       "n_layers              -0.0033      0.007     -0.457      0.650      -0.018       0.011\n",
       "n_neurons           4.884e-06   1.81e-06      2.700      0.009    1.27e-06     8.5e-06\n",
       "act_func_LeakyReLU     0.0154      0.009      1.766      0.082      -0.002       0.033\n",
       "act_func_ReLU          0.0094      0.008      1.138      0.259      -0.007       0.026\n",
       "optimizer_Adadelta    -0.0161      0.007     -2.249      0.028      -0.030      -0.002\n",
       "optimizer_Adagrad     -0.0029      0.006     -0.466      0.643      -0.015       0.009\n",
       "optimizer_Adam         0.0183      0.005      3.561      0.001       0.008       0.029\n",
       "optimizer_Nadam        0.0255      0.006      4.090      0.000       0.013       0.038\n",
       "==============================================================================\n",
       "Omnibus:                        1.799   Durbin-Watson:                   1.624\n",
       "Prob(Omnibus):                  0.407   Jarque-Bera (JB):                1.359\n",
       "Skew:                          -0.321   Prob(JB):                        0.507\n",
       "Kurtosis:                       3.107   Cond. No.                     2.52e+19\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 7.32e-31. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping as these are dummy variables\n",
    "trials_df = trials_df.drop(['act_func_LeakyReLU', 'optimizer_Adagrad'], axis=1)\n",
    "trials_df.iloc[:,0] = trials_df.iloc[:,0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "params = pd.DataFrame()\n",
    "tvalues = pd.DataFrame()\n",
    "\n",
    "for quantile in [0.1,0.25,0.5,0.75,0.9]:\n",
    "    # print(f'\\n\\n QUANTILE: {quantile}\\n\\n')\n",
    "    y_var = trials_df.iloc[:,0]\n",
    "    mod = smf.quantreg(f\"y_var~ {' + '.join(trials_df.columns[1:])}\", trials_df)\n",
    "\n",
    "    res = mod.fit(q=quantile)\n",
    "    params_temp = pd.DataFrame(res.params).reset_index(drop=False).rename({0:'Q'+str(int(quantile*100))}, axis=1)\n",
    "    tvalues_temp = pd.DataFrame(res.tvalues).reset_index(drop=False).rename({0:'Q'+str(int(quantile*100))}, axis=1)\n",
    "    \n",
    "    if quantile == 0.1:\n",
    "        params = pd.concat([params, params_temp])\n",
    "        tvalues = pd.concat([tvalues, tvalues_temp])\n",
    "\n",
    "    else:\n",
    "        params = params.merge(params_temp, on='index')\n",
    "        tvalues = tvalues.merge(tvalues_temp, on='index')\n",
    "\n",
    "    # params.append(res.params)\n",
    "\n",
    "    # print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Q10</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q50</th>\n",
       "      <th>Q75</th>\n",
       "      <th>Q90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>5.361243</td>\n",
       "      <td>-3.620778</td>\n",
       "      <td>2.944926</td>\n",
       "      <td>7.932799</td>\n",
       "      <td>8.317492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>learning_rate</td>\n",
       "      <td>-0.000893</td>\n",
       "      <td>-0.058347</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>-0.015400</td>\n",
       "      <td>0.022984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l1_lambda1</td>\n",
       "      <td>-2.305678</td>\n",
       "      <td>-3.457612</td>\n",
       "      <td>-1.969190</td>\n",
       "      <td>-1.226784</td>\n",
       "      <td>-5.078277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l2_lambda</td>\n",
       "      <td>3.381082</td>\n",
       "      <td>5.271729</td>\n",
       "      <td>0.192328</td>\n",
       "      <td>-1.396716</td>\n",
       "      <td>4.041936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dropout_prob</td>\n",
       "      <td>-3.084483</td>\n",
       "      <td>-2.949126</td>\n",
       "      <td>-1.674480</td>\n",
       "      <td>-1.094271</td>\n",
       "      <td>-1.230989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>batch_norm</td>\n",
       "      <td>-0.863580</td>\n",
       "      <td>0.454351</td>\n",
       "      <td>0.168629</td>\n",
       "      <td>0.210675</td>\n",
       "      <td>-0.292060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n_layers</td>\n",
       "      <td>-1.330104</td>\n",
       "      <td>0.985993</td>\n",
       "      <td>-0.158887</td>\n",
       "      <td>-0.579317</td>\n",
       "      <td>-0.606902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>n_neurons</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>act_func_ReLU</td>\n",
       "      <td>-1.801242</td>\n",
       "      <td>-0.685129</td>\n",
       "      <td>-0.721604</td>\n",
       "      <td>0.143039</td>\n",
       "      <td>-0.168252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>optimizer_Adadelta</td>\n",
       "      <td>-0.798102</td>\n",
       "      <td>-0.240359</td>\n",
       "      <td>-1.316220</td>\n",
       "      <td>-3.737082</td>\n",
       "      <td>-1.163615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>optimizer_Adam</td>\n",
       "      <td>2.092094</td>\n",
       "      <td>4.515817</td>\n",
       "      <td>3.407059</td>\n",
       "      <td>0.579873</td>\n",
       "      <td>0.883671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>optimizer_Nadam</td>\n",
       "      <td>5.349798</td>\n",
       "      <td>5.734817</td>\n",
       "      <td>3.556814</td>\n",
       "      <td>0.827675</td>\n",
       "      <td>0.846301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index       Q10       Q25       Q50       Q75       Q90\n",
       "0            Intercept  5.361243 -3.620778  2.944926  7.932799  8.317492\n",
       "1        learning_rate -0.000893 -0.058347 -0.007902 -0.015400  0.022984\n",
       "2           l1_lambda1 -2.305678 -3.457612 -1.969190 -1.226784 -5.078277\n",
       "3            l2_lambda  3.381082  5.271729  0.192328 -1.396716  4.041936\n",
       "4         dropout_prob -3.084483 -2.949126 -1.674480 -1.094271 -1.230989\n",
       "5           batch_norm -0.863580  0.454351  0.168629  0.210675 -0.292060\n",
       "6             n_layers -1.330104  0.985993 -0.158887 -0.579317 -0.606902\n",
       "7            n_neurons  0.000471  0.000265  0.000387  0.000186  0.000295\n",
       "8        act_func_ReLU -1.801242 -0.685129 -0.721604  0.143039 -0.168252\n",
       "9   optimizer_Adadelta -0.798102 -0.240359 -1.316220 -3.737082 -1.163615\n",
       "10      optimizer_Adam  2.092094  4.515817  3.407059  0.579873  0.883671\n",
       "11     optimizer_Nadam  5.349798  5.734817  3.556814  0.827675  0.846301"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('/home/ge65cuw/thesis/saved/final_results/results_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x468 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),  \n",
    "            (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),  \n",
    "            (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),  \n",
    "            (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),  \n",
    "            (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]  \n",
    "color_index = 18\n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.  \n",
    "for i in range(len(tableau20)):  \n",
    "    r, g, b = tableau20[i]  \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)  \n",
    "path_top = path + '_feature_importance_top20.png'\n",
    "path = path + '_feature_importance.png'\n",
    "\n",
    "# x_pos = (np.arange(max['feature'])))\n",
    "\n",
    "fig = plt.figure(figsize=(12,6.5))\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(3,2,1)\n",
    "# ax = plt.axes()\n",
    "# ax.set_title('Feature Importance', fontsize=25)\n",
    "# ax.set_xticks(x_pos)\n",
    "\n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"bottom\"].set_visible(True)  \n",
    "ax.spines[\"right\"].set_visible(False)  \n",
    "ax.spines[\"left\"].set_visible(True)  \n",
    "# ax.set_xticklabels(results_df['FF5_Mom_STRev_alpha_VW'], rotation=90, ha='center', fontsize=12)\n",
    "# ax.bar(results_df['FF5_Mom_STRev_alpha_VW'],align='center', zorder=3, color=tableau20[color_index], height=1)\n",
    "ax.set_xlabel('7 Factor Model alpha')\n",
    "ax.hist(results_df['FF5_Mom_STRev_alpha_VW'], density=True, color=tableau20[color_index], align='mid', zorder=3)\n",
    "# plt.margins(y=0.01, x=.005)\n",
    "ax.xaxis.grid(True, linestyle='--',  zorder=0)\n",
    "ax.yaxis.grid(True, linestyle='--',  zorder=0)\n",
    "\n",
    "ax = fig.add_subplot(3,2,2)\n",
    "\n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"bottom\"].set_visible(True)  \n",
    "ax.spines[\"right\"].set_visible(False)  \n",
    "ax.spines[\"left\"].set_visible(True)  \n",
    "color_index = 16\n",
    "ax.hist(results_df['oosSpearman'], density=True, zorder=3, color=tableau20[color_index], align='mid')\n",
    "ax.set_xlabel('Out of Sample Spearman Coefficient')\n",
    "# plt.margins(y=0.01, x=.005)\n",
    "ax.xaxis.grid(True, linestyle='--',  zorder=0)\n",
    "ax.yaxis.grid(True, linestyle='--',  zorder=0)\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4804d781847468e0794096453e716a44bf82c4d93fcc1a726a520e37d704781"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
